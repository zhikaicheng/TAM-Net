import tensorflow as tf

class ResidualBlock(tf.keras.layers.Layer):
    def __init__(self, hidden_dim):
        super(ResidualBlock, self).__init__()
        self.dense1 = tf.keras.layers.Dense(hidden_dim, activation='relu')
        self.dense2 = tf.keras.layers.Dense(hidden_dim)
        self.relu = tf.keras.layers.ReLU()
        
    def call(self, x):
        residual = x
        x = self.dense1(x)
        x = self.dense2(x)
        x += residual
        return self.relu(x)

class AttentionModule(tf.keras.layers.Layer):
    def __init__(self, hidden_dim):
        super(AttentionModule, self).__init__()
        self.dense = tf.keras.layers.Dense(hidden_dim, activation='relu')
        self.attention_weights = tf.keras.layers.Dense(hidden_dim, activation='softmax')
        
    def call(self, x, prev_attention=None):
        if prev_attention is not None:
            x = x + prev_attention  # 前面的注意力模块影响后面的
        x = self.dense(x)
        attention = self.attention_weights(x)
        return x * attention, attention

class SharedNetwork(tf.keras.layers.Layer):
    def __init__(self):
        super(SharedNetwork, self).__init__()
        self.input_layers = [tf.keras.layers.Dense(128, activation='relu') for _ in range(4)]
        self.fc_layers = [tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dense(64, activation='relu')]
        self.residual_blocks = [ResidualBlock(128) for _ in range(3)]
        self.final_fc = tf.keras.layers.Dense(64, activation='relu')

    def call(self, inputs):
        encoded_features = [layer(x) for layer, x in zip(self.input_layers, inputs)]
        x = tf.concat(encoded_features, axis=-1)
        
        for layer in self.fc_layers:
            x = layer(x)
        
        for block in self.residual_blocks:
            x = block(x)
        
        return self.final_fc(x)

class MTANModel(tf.keras.Model):
    def __init__(self):
        super(MTANModel, self).__init__()
        self.shared_network = SharedNetwork()
        
        # 多任务的注意力模块
        self.attention_modules = [
            AttentionModule(64),
            AttentionModule(64),
            AttentionModule(64)
        ]
        
        # 任务专属网络
        self.task_networks = [tf.keras.Sequential([
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dense(1)
        ]) for _ in range(4)]

    def call(self, inputs):
        # 共享主网络
        shared_features = self.shared_network(inputs)
        
        # 应用注意力模块并进行连接
        attention_output, attention_prev = self.attention_modules[0](shared_features)
        for i in range(1, len(self.attention_modules)):
            attention_output, attention_prev = self.attention_modules[i](attention_output, attention_prev)
        
        # 分别使用不同的任务网络进行预测
        outputs = [task_net(attention_output) for task_net in self.task_networks]
        return outputs
